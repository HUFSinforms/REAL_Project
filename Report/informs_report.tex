%%%%%%%%%%%%
\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{textcomp}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}
\parindent0pt
\usepgfplotslibrary{statistics}
\parskip\bigskipamount

\begin{document}

\centerline{\textbf{2018 INFORMS O.R. {\&} Analytics Student Team Competition -- ENTRY FORM}}

\baselineskip16pt plus 1pt minus 1pt


\textbf{Entry Number: [}2018ORASTC252]

\textbf{Executive Summary (not to exceed 2 pages)}\\*


\textbf{Team Makeup {\&} Process}\\*

\textbf{Framing the Problem}\\*
%Rebalancing을 위한 로직 (turnover 반영)
%파라메터 $\gamma$ 반영방법?!
%Robustness


\textbf{Data}\\*
\begin{itemize}
\item[1.] The Structure of the Data %데이터 구조 및 분석
\item[] The data from Principal consists of 3 parts which are timeseries data, riskmodels data, and result template data. 
\begin{itemize}
	\item Time Series Data :
	\begin{itemize}
		\item[] 
	\end{itemize}
	\item Riskmodels Data :
	\begin{itemize}
		\item[] 
	\end{itemize}
	\item Result Template Data :
	\begin{itemize}
		\item[] 
	\end{itemize}
\end{itemize}

\item[2.] Data Pre-processing and Rescailing

	% 왜 data preprocessing이 필요한가. 어떤 방식으로 하였는가 (전반적)
	\item[ ] There are some difficulties in applying the data given from Principal directly to the model. Therefore, we present the following data preprocessing process. To make the data composed of three parts more flexible, a process of data preprocessing formed dataframe using the Pandas Library(http://pandas.pydata.org) in Python. 
	
	% Timeseries를 가지고 있는 Time Series Data 및 Result Template Data를 날짜별로 추출
	In particular, as rebalancing is carried out, it is configured to extract not all time series data, but only the data needed for the iteration. The parameters used in model were extracted from data frames formed for each period, when each parameter (Alpha Score($\alpha$), Beta($\beta$),4 Weekly Returns ($r$), etc) was set with the 'SEDOL' index as the key value. This dictionary data type is proper to consider a list of assets that may change every period.
	
	% Riskmodel Data에서 Omega(Risk Matrix)를 추출하는 방식
	The parameter Omega ($ \Omega $) is specified as the covariance matrix for a given data. In this case, the index in the columns and rows are 'SEDOL', the key value of the time series dictionary derived from the above, and are constructed in the form of full matrix. 
	
	% Quadratic 을 표현하기 위해 필요한 정제 (????)
	
	%Data Rescailing
	%\omega * \omega 와 alpha score(범위 : -2.08e-05~1.93e-05) 의 값이 너무 작음 --> Python의 한계수치로 인한 계산 오류 발생 가능성 및  Optimization Tool CPLEX에서 값이 무시되는 경우 가 생길 수 있음.
	
\item[3.] The Analysis of the Data
	% MCAPQ 와 SECTOR별 Weight의 범위 (결과?와 연관지어서)
	% Active Share과 Tracking Error의 관계 (데이터 기반)
	% 추가 historical 데이터 확보 및 분석 --> omega 도출 결과 corelation이 낮음 => 이유 간단히 분석?
	
\end{itemize}

\textbf{Methodology Approach {\&} Model Building}\\*
\begin{itemize}
\item[\textbf{1.}] \textbf{Formulation Approach}
	\item[] \underline{\textbf{Model Building}}:
	% CPLEX를 가지고 QCQP 풀었을 경우 (1) Linearlize (2) Handling non-convex constraints -> Bisearch Approach (3)간단한 실험 (4)실험결과 분석 (문제를 어렵게 만드는 제약조건 , cardinality 제약식)
	Before buiding the model, we tried to solve the  Quadratic Constraint Quadratic Programming (QCQP)  problem by using the IBM CPLEX, which is well known Optmization tool, to determine the difficulty of the problem. Since CPLEX can not deal with non-linear constraints,the given formulation is required to be reformulated. 
	First, some constraitns could be reformulated as follows:
	\begin{align*}
	\text{(MIP)} \quad \quad &(1) \sim (8) \\
    & y_i \geq w_i, \quad \forall i \in N    \quad \quad \quad  \quad  \quad  \quad\text{(9*)}\\
	& y_i \leq w_i + 0.999 \quad \forall i \in N \\
	& 50 \leq \sum_{i \in N} y_i \leq 70 \\
	& y_i \in \{0,1\}, \quad \forall i \in N \\
	%& \frac{1}{2} \sum_{i \in N}|w_i - w_{\text{bench i}}|  \leq 1  \quad \quad \quad (10*) \\
	&  d^T\Omega  d \  \leq 0.01  \quad  \quad  \quad  \quad \quad \ \ \quad \quad(11*)
	\end{align*} 
	The decision variables $y_i$ is 1 if asset $i \in N$ is selected where $w_i$ is bigger than 0. Because $w_i \leq 0.001$ is considered to $w_i = 0$,  $y_i = 0$ if $w_i = 0$. The opposite is the same as well. Constraints (10) and (11) are also non-linear. However, constraints(10) and (11) can not be easily linearized unlike constraitns(9). However, if non-convex constraints are linearized, they significantly impact speed by increasing the number of variables to determine and the number of constraints to consider. In other words, it is very inefficient to reformulate constraints (10) and (11), which represent the limits of Active Share and Tracking Error, respectively. Therefore, the experiments are conducted to solve MIP which did not consider the constraints (10) and the left side of the constraints (11), and then we check whether the best feasible solution from MIP violate limit range of Tracking Error and Active Share. The first date was used for the experiements.  
	
\begin{table}[h]
	\centering
	\footnotesize 
	\caption{The results of solving MIP by Cplex}
	\label{tab1}
	\begin{tabular}{cccccc}
\\
		\hline
		\hline
		\textbf{B\&B time (sec)} & \textbf{\# of feasible} & \textbf{GAP(\%)} & \textbf{Tracking Error} & \textbf{Active Share} & \textbf{Violated Constraints} \\ [2mm]\hline
		\textbf{30} & 0 & - & - & - & -  \\[2mm]
		\textbf{60} & 0 & - & - & - & -  \\[2mm]
		\textbf{90} & 2 & 100.3\% & {\color[HTML]{CB0000} 0.0018} & 0.72 & (11) TE \\[2mm]
		\textbf{120} & 2 & 100.3\% & {\color[HTML]{CB0000} 0.0012} & 0.64 & (11) TE \\[2mm]
		\textbf{150} & 2 & 100.3\% & {\color[HTML]{CB0000} 0.0018} & 0.72 & (11) TE \\[2mm]
		\textbf{180} & 8 & 100.6\% & {\color[HTML]{CB0000} 0.0004} & {\color[HTML]{CB0000} 0.50} & (10)AS, (11) TE  \\ \hline
	\end{tabular}
\end{table}
	Table 1 shows the number of feasible solutions found by branch and bound for a given time and several indicators of the best feasible solution among feasible solutions of MIP. In particular, GAP represents the difference (\%) from the best integer solution and it can be confirmed that no good solution is found within 3 minutes. In terms of finding a feasible solution, the following obeservations were derived.
	\begin{itemize}
		\item[] \textbf{Observation1.} In most cases, the range of Active Share is satisfied.
		\item[] \textbf{Observation2.} Tracking Error is less than lower bound (0.05).
		%\item[] \textbf{Observation3.} The portfolio weights($w$) tend to be less than the benchmark weights($w_{\text{bench}}$).
	\end{itemize}
%Tracking Error가 왜 낮은지? obj val 을 낮추려 하니깐...!
	%According to Observation 2, Tracking Error should be raised. At this time, $d^T\Omega d$ can be expressed as $\quad w^T\Omega  w - w_{\text{bench}}^T\Omega  w_{\text{bench}} \quad$, which means that the difference between the benchmark weight and the portfolio weight is too small that Tracking Error does not satisfy the lower bound. In other words, the difference between the two weights must be increased to some extent to satisfy the constraints.
	
	
	 %The bisectional search algorithm reflects that if some portfolio weights are increased, the othe portfolio weights of the assets become smaller because of the constraints(4). Since the following $  w_{\text{bench}}^T\Omega  w_{\text{bench}}  $ is fixed, it is required to satisfy the constraints by changing $w^T\Omega  w $. Because the weight value of all of the unselected assets from approximately 500 candidates is zero, the weight value is less than the benchmark weight value ($\because w_{bench} > 0$). On the other hand, the most of weight value of the 50 to 70 selected assets may be larger than the benchmark weights. Thus, we took a look for the average of portfolio weight and have noticed that about a third is smaller than average and two-thirds is larger than the average. Therefore, based on the average of portfolio weight, the portfolio weights are raised for the assets whose portfolio weight are larger than the average. Conversely, the assets whose portfolio weights are smaller than the average are decreased. As a result, the Tracking Error value becomes large, making the solution feasible.
	According to Observation 2, Tracking Error should be raised. For the given data, benchmark weights are distributed over about 500 assets, while the portfolio weight is 0 $(w_i = 0)$ for most of the remaining assets except for 50 to 70 selected assets. Thus, for non-selected assets $i$, $d_i$ can be expressed as $-w_{\text{bench}}$. In other words, if the assets whose $w_{\text{bench}}$ are large are not selected, the $\sum_{i \ inN}d_i$ becomes larger which means the Tracking Error is increased. %데이터특성상 bw가 작아서 이거로 키우는데는 무리가 있지 않을까 실질적으로 선택된 자산들을 조정하는게 효율적
	On the other hand, for the selected 50 to 70 assets, most $w_i $ are larger than $w_{\text{bench}}$. If the difference between $w_i$ and $w_{\text{bench}}$ is small, $ \sum_{i \ inN}d_i$  is also small. The difference between $w_{\text{bench}}$ and $w_i$ are required to be large for all selected assets,  so $ \sum_{i \ inN}d_i$ becomes larger when the difference between $w_{\text{bench}}$ and $w_i$ are large for all assets $i$. That is, the tracking error also becomes large. First, We took a look for the average of portfolio weight and have noticed that about two-thirds is smaller than average and a third is larger than the average. Therefore, based on the average of portfolio weight, the portfolio weights are raised for the assets whose portfolio weight are smaller than the average. Conversely, the assets whose portfolio weights are larger than the average are decreased. As a result, the Tracking Error value becomes large, making the solution feasible. To sum up, there are the ways to increase the tracking error by not selecting the assets with a larger $w_{\text{bench}}$ or by increasing the difference between $w_{\text{bench}}$ and $w_i$ for the selected assets. Since the value of the bench weight is practically insignificant, the method which increases the difference between $w_{\text{bench}}$ and $w_i$ for the selected assets is more efficient. 
	
	
	
	%The bisectional search algorithm reflects that if some portfolio weights are increased, the othe portfolio weights of the assets become smaller because of the constraints(4). Since the following $  w_{\text{bench}}^T\Omega  w_{\text{bench}}  $ is fixed, it is required to satisfy the constraints by changing $w^T\Omega  w $. Because the weight value of all of the unselected assets from approximately 500 candidates is zero, the weight value is less than the benchmark weight value ($\because w_{bench} > 0$). On the other hand, the most of weight value of the 50 to 70 selected assets may be larger than the benchmark weights. Thus, we took a look for the average of portfolio weight and have noticed that about a third is smaller than average and two-thirds is larger than the average. Therefore, based on the average of portfolio weight, the portfolio weights are raised for the assets whose portfolio weight are larger than the average. Conversely, the assets whose portfolio weights are smaller than the average are decreased. As a result, the Tracking Error value becomes large, making the solution feasible. 


	\item[] \underline{\textbf{Bisectional Search Algorithm}}:
	 We applied the idea to the bisectional search algorithm. The bisectional search algorithm is an iterative algorithm that narrows the range and finds a feasible solution. When narrowing the range, if the feasible solution exists in the range, search in the right range based on the median of the range, and search in the left range if the feasible solution does not exist.	
	An example is shown in \ref{fig:bisection}. Let  $ \sum_{p\in P} w_p = 0.6$, where $P$ is a set of assets whose $w_i$ is smaller than the avarage of the portfolio weights. First, the solution is searched by dividing the interval between 0.8 and 1.0. If the solution is a feasible solution that satisfies both the range of Active Share and Tracking Error, narrow the range and search again in the left section. The search is stopped when the width of the range($\underline{Z} \sim \overline{Z}$) is less than the end condition which is set to 0.001. One of the feasible solutions, obtained from the bisectional search with the lowest objective value, is returned. Tracking Error increases as $\underline{Z}$ approaches to 1. Conversely, Tracking Error decreases as the $\overline{Z}$ approaches to $\sum_{p\in P}w_p$ (0.6 in the example). 
	\begin{figure}[h] 
		\begin{center}
			\includegraphics[width=0.75\textwidth]{bisection}
			\caption{An Example of Bisectional Search} \label{fig:bisection}
		\end{center}
	\end{figure}
	Therefore, the following constraints are derived through the bisectional algorithm : $\sum_{p \in P} w_i = (\underline{Z} +\overline{Z})/2 $ . Experimental results including the constraints show that Tracking Error is converged at the lower bound (0.05), except for the case where the difference between Tracking Error value and the lower bound is large before the start of the bisectional search.
	
	% bisectional algorithm 결론
	
	%\item[]\underline{Evolutionary Approach} : 
	
	% 진화 알고리즘 (ex.EDA)으로 접근. (간단한 실험?) -> 문제점 도출 : population의 수를 크게 두고 여러번 풀어 확률적 이론 등을 적용시키는 데에는 오랜시간 소요.
		
	\item[\textbf{2.}] \textbf{Neural Network Approach}

	\item[]\underline{\textbf{The application of GAN}} : \textbf{Generative adversarial networks (GANs)} are well known deep neural net(NN) architectures comprised of two networks. One neural network, called the  \textbf{Generator(G)}, generates new data instances, while the other, the  \textbf{Discriminator(D)}, decides whether each instance of data is real dataset or not. Both neural networks learn to alternate with each other. The G trains to generate data similar to real data, and the D trains to distinguish real data from fake data generated by G. 

		\begin{figure}[h] 
		\begin{center}
			\includegraphics[width=0.6\textwidth]{GAN_basic}
			\caption{The strucutre of basic GAN} \label{fig:GAN-basic}
		\end{center}
	\end{figure}

See Figure 2. G generates fake data when arbitrary noise z is given as a input through a NN composed of layers which are a input layer, multiple hidden layers and a output layer. G tries to make $D(x) = 1$ for sample x extracted from fake data, and D tries to make $D(G (z)) = 0$ for sample $G (z)$ , where $z \sim p_{z}(z)$. Therefore, the loss function $V(D,G)$ of GAN can be expressed as follows : %reference?
 \begin{align*}
 \min_G \max_D V(D,G)  = \mathbb{E}_{x~p_{data}(x)}[\log D(x)]  + \mathbb{E}_{x~p_{x}(x)}[\log(1-D(G(z)))] 
 \end{align*}
 We use the GAN 's basic idea of learning networks with adversarial relationships between two networks to solve the problem of portfolio optimization. G determines the decision variable $w$ of the formulation, and D determines if the data generated from G is feasible. D is based on the formulation, and the sample data $G(z)$ generated checks both the feasibility of the solution and the quality of objective value. Specifically, when $ w_i $ is determined in G, the discriminator finds the sum of the objective value and the scaled value of the sum of values deviating from each constraint. Therefore, the value of loss in D is the sum of the degree of violation from the constraints and objective value. G is a deep NN structure consisting of one input layer, three hidden layers and one ouput layer. G tries to minimize the loss of D for the sample $G(z)$ generated from the random input noise $z \sim \mathcal {U}(-1,1)$. That is, the decision variable $ w $ generated in G is trained to be feasible for all constraints and the objective value to be minimized. Unlike the basic GAN structure, GAN for portfolio optimization has G for NN structure, but D is not, and only G is required to train. The loss function $V(G)$ of GAN can be expressed as follows :
 \begin{align*}
 \max_G \mathbb{E}_{x~p_{data}(x)}[\log(D(G(z)))] 
 \end{align*}
 The training is performed with a total of 2000 random datasets so that the loss value is minimized. At this time, the batch size is 1000 datasets, and then sample data is extracted after training 1000 datasets. At this time, if the value of D (G (z)) is larger than the threshold value (0.995) for sample data G (z), the solution is considered as feasible. 
 %Figure 3 shows the structure of G composed of NN and D composed of Formulation.
In order to understand whether GAN is being well trained, we let GAN train with 2000 random datasets and examined the results. As a result of the experiments, we realized some of constraints are violate. However, we also realized that only about 5 percent of the constraints of the whole are violated, and the degree of violation is very small. Because of threshold, the results of trained GAN allow in some degree of infeasible. 


%내용 추가 및 간단한 실험??????

\begin{figure}[h] 
	\begin{center}
		\includegraphics[width=0.6\textwidth]{GAN_port}
		\caption{The strucutre of GAN for portfolio optimization} \label{fig:GAN-port}
	\end{center}
\end{figure}
	
\item[\textbf{3.}] \textbf{GAN-MP Hybrid Approach} %이름 
	\item[] We propose a new \textbf{GAN(Generative adversarial networks)-MP(Mathmatical Programming) Hybrid approach} that complements the disadvantages of formluation approach and neural network approach and enhances their advantages. Several limitations could be derived from the three approaches we present above. When solving the problem with the formulation appraoch, we could not find a reasonable solution because to consider about 500 assets per iteration the number of decision variables to decide and the number of constraints to be satisfied are too high . Especially, constraints (9) which select cardinalities considering the target range on number of stocks make the problem difficult. 
	In the case of Neural Network Approach, there is a great advantage in that a solution that does not get significantly out from the whole constraints and that has a good objective value can be considered at the same time. However, it is difficult to find a soluton satisfying the feasibility for all the constraints and the fact that the training time takes too much time. The GAN-MP hybrid approach is a highly efficient approach that maximizes the advantages of GAN and formulation-based optimizations. This appraoch consists of three steps as shown in Figure 4. Step1 is the process of finding the cardinality set which is the combination of the selected assets. The cardinality set selected in Step1 solves the QCQP problem with the selected assets fixed and finds the local optimal solution in Step2. According to the result of Step2, the update algorithm finds cardinality set that can reduce the objective value and improves the solution.
	\begin{figure}[h] 
		\begin{center}
			\includegraphics[width=0.45\textwidth]{flowchart}
			\caption{A Flowchart of Hybrid Approach} \label{fig:flowchart}
		\end{center}
	\end{figure}

	\item[]\underline{\textbf{(Step1) Select Initial Cardinality Set \& (Step2) Solve the QCQP }} :
	 When the number of assets to be considered is large, the problem becomes large as the number of constraints increases as well as decision variables. 
	 Thus, the process of step1 filters out good assets that are crucial for minimizing the objective value. At this time, the set of good assets must meet the following criteria.
	 \begin{itemize}
	 	\item[]\textbf{Criterion 1.} The combination of assets should be satisfy all constraints
	 	\item[]\textbf{Criterion 2.} The combination of assets is chosen to minimize the risk and maximize return.
	 \end{itemize}
	  In order to meet these criteria , the whole assets are required to be considered. We propose a formulation approach that finds the solution through Mixed Integer Programming (MIP) and a neural network-based GAN as a way to satisfy constraints and to take low values of objective value globally. Therefore, the experiments are carried out with two algorithms (MIP, GAN). For a given time, the MIP proceeds branch and bound(B\&B) and the GAN proceeds to train the generator consisting of NN (Step1). After the given time, the assets, $w_i> 0.001$ for all asset $i \in N$ ,were extracted in the best solution found in each algorithm. The portfolio optimization problem that determines the weight of each assets as the cardinality set is fixed (Step2) . Accordingly, constraints (9) does not to be considered anymore, and the set $N$ changes to $S$, which stands for set of selected assets for all of the constraints. Therefore, the number of constraints and decision varaibles are reduced. 
	 % The following experiments were conducted to evaluate the performance of the algorithms.
	 \begin{itemize}
	 	\item Experiment Environment
	 	\begin{itemize}
	 		\item Number of Experiments : 50 times
	 		\item The B\&B Timelimit for MIP : 90 seconds  \begin{scriptsize}	(minimun time for feasible solution)	\end{scriptsize}
	 		\item Training Time for GAN : 20 seconds
	 		\item Solver of MIP : Cplex
	 		\item Deep Learning Library : Pytorch
	 		\item Data of the first period (2007/01/03)
	 	\end{itemize}
 		\item Experiment Results :
	 \begin{figure}[h]
	 \begin{center}
	 \begin{tikzpicture}[scale=0.93]
	 \begin{axis}[
	 ybar,
	 enlargelimits=0.15,
	 legend style={at={(0.5,-0.20)},
	 	anchor=north,legend columns=-1},
	 ylabel={\# of solutions},
	 symbolic x coords={MIP,GAN, Random},
	 xtick=data,
	 nodes near coords,
	 nodes near coords align={vertical},
	 	title={\large\textbf{(a) Feasibility of Algorithms}}
	 ]
	 \addplot coordinates {(MIP,0) (GAN,35) (Random,17)};
	 \addplot coordinates {(MIP,50) (GAN,15) (Random,33)};
	 \legend{\# of Feasilbe$\quad$, \# of Infeasible}
	 
	 \end{axis}
	 \end{tikzpicture} 
	 	\quad
	 \begin{tikzpicture}[scale=1.]
	 \begin{axis}
	 [
	 ytick={1,2},
	 yticklabels={\footnotesize GAN, \footnotesize Random},
	 xlabel={\small Objective Value  },
	 	title={\small\textbf{(b) Objective Values of Algorithms}}
	 ]
	 \addplot+[
	 boxplot prepared={
	 	median=25.14,
	 	upper quartile= 27.27,
	 	lower quartile=25.056,
	 	upper whisker=36.43,
	 	lower whisker=25.013
	 },
	 ] coordinates {};
	 \addplot+[
	 boxplot prepared={
	 	median=33.66,
	 	upper quartile=40.18,
	 	lower quartile=27.90,
	 	upper whisker=43.58,
	 	lower whisker=25.26
	 },
	 ] coordinates {};
	 
	 \end{axis}
	 \end{tikzpicture}
	 			\caption{Results of Algorithms} \label{fig:feasi}
	 	 \end{center}
 	 	\end{figure}
  	\item[]  The cardinality sets are derived by solving MIP problem with the Cplex, and  all of the results were infeasible when solving the portfolio optimization problem with the fixed assets. One reason that cannot derive the feasible solutions is that Tracking Error is too low because the constraints of Tracking Error are not reflected in MIP. Thus, Cplex does not suit for selecting initial cardinality set.  If random samples are specified as an initial cardinality set, the solutions are often infeasible, and objective values are lower than GAN. \underline{Therefore, the initial cardinality set is assigned to the GAN} because the feasible solution is derived more reliably than others, and the objective value is relatively low. 

	 \item Threshold and Suggestion :
	 
	 GAN is composed of NNs, and the result quality and training time vary depending on computing environment of NN. Therefore, we recommend that Principal could improve the training speed and quality by using GPU-accelerated computing or through some advanced technologies such as Amazon Cloud services. By improving the environment, more accurate and better solutions are derived by adjusting the hidden layer addition and the learning rate that we could not reflect now because of computing power. 
 
 	 \end{itemize}
 
 
 
\begin{figure}[h] 
	\begin{center}
		\includegraphics[width=0.6\textwidth]{step1}
		\caption{A Flowchart of Hybrid Approach} \label{fig:step1}
	\end{center}
\end{figure}


	We present a model that can maximize the benefits of GAN in a given situation. The major advantage of the GAN is that it can extract solutions whenever noises are given to G by learning the weight and bias of the generator.
	The server that currently runs the code has eight cores, so we figure out the server can run up to 8 GANs simultaneously on 8 cores. Therefore, we use the parallel process to be 8 GANs training at the same time and then obtain 8 initial cardinality sets each as a result. The parallel process uses the Python Library of Ipython Parallel (https://ipyparallel.readthedocs.io), and Figure 6 shows the process of obtaining 8 sets of cardinality sets by running 8 GANs using Ipython Parallel.

	\item[]\underline{\textbf{(Step3) Update Algorithm }} :  The assets to be included in the portfolio are selected by GAN, and the MIP is solved by Cplex for determining weights to each assets. 
	As a result of solving QCQP problems, the solutions are derived. At this time, the local optimal solution may be improved depending on which asset is selected. Therefore, based on good initial cardinality set, it is necessary to perform a local search by changing the items of the asset, the weight of the asset, and the number of selected assets. However, if the initial cardinality set obtained from the GAN is not good in Step 1, changing the number of assets does not improve the result significantly. To determine the bad cardinality set, we check the quality of the solution when we solve the MIP with the initial cardinality set before updating the cardinality set. The following process is performed according to the result of 8 solutions. For 4 cardinality sets with bad results, it is required to find a new initial cardinality set ($\rightarrow$ Step1. Select Initial Cardinality Set). For 4 the other cardinality sets with good results, it adjusts the assets to improve results ($\rightarrow$ Step3. Update Algorithm). 
	
	%Update Algorithm
	
	\begin{figure}[h] 
		\begin{center}
			\includegraphics[width=0.65\textwidth]{step3}
			\caption{A Flowchart of Hybrid Approach} \label{fig:step3}
		\end{center}
	\end{figure}
	
	
	 Basically, update algorithm has 2 goals. 
	\begin{itemize}
		\item[(1)] One is that the update algorithm finds one or multiple assets whose objective value is lower when one or multiple assets is/are added or removed. 
		\item[(2)] The other is if the result of current cardinality set is infeasible, it also adjusts the cardinality set so that it becomes feasible.
	\end{itemize}
	To accomplish these two goals, the update aglrithm is supposed to adjust the cardinality set solving the problem and improving the solution until it is reachead to the end condition. First, the update algorithm finds out which asset gets better to lower the objective value when it gets out of the portfolio, and which asset should be added to improve the outcome. From the viewpoint of determining which asset is included in the portfolio, there is a great difficulty in deriving an objective value that can be obtained only when the weight of each asset is determined. We therefore take into account the maximum value that an asset has on the determination of the objective value. In the objective function, the risk is minimized while the return is maximized. Let $C$ is a set of assets whose asset $i$ is in cardinality set, and $K$ is a set of assets whose asset is not in cardinality set, that is candidate set for update algorithm. In order to find an good asset with higher return and lower risk, return and risk for the candidate asset $k \in K$ are calculated as follows:
	\begin{itemize}
		\item[\textbullet] Return = The maximum return that occurs as the asset $k$ comes in $\Rightarrow \alpha_k$
		\item[\textbullet] Risk = The maximum risk that occurs as the asset $k$ comes in $\Rightarrow \sum_{c \in C}\Omega_{ck} + \Omega_{kk}$
	\end{itemize}
	On the contrary, the asset $c$\textasciiacute$\,$ that increases the objective value because the risk is large and the return is small among the assets included in the current cardinality set is also determined by the following criteria : 
	\begin{itemize}
	\item[\textbullet] Return = The maximum return that decreases as the asset $c$\textasciiacute$\,$ leaves the cardinality set $\Rightarrow \alpha_{{c}'}$
	\item[\textbullet] Risk =  The maximum risk that decreases as the asset $c$\textasciiacute$\,$ leaves the cardinality set $\Rightarrow \sum_{c \in \{c | c \in C \, \text{and} \, c \neq \ {c}' \}}\Omega_{c {c}'} + \Omega_{{c}' {c}'}$
	\end{itemize}
	Therefore, in order to improve the objective function, the update algorithm minimizes the Risk $-$ Return for the new candidate asset $k$ and eliminates the asset $c$\textasciiacute$\,$ which have a bad effect on the existing cardinality set. Repeating this process can result in a local optimal solution as a result of obtaining a good cardinality set. However, infeasible cardinality sets can be obtained in many cases when considering the risk and return to obtain a good cardinality set. The cardinality set is infeasible, which means that when a QCQP problem is solved after fixing an asset with a cardinality set, a feasible solution can not be found in violation of some constraints. We have noticed that most infeasilbe cardinality sets occur in Constraints 7 and 8 indicating the limits of Sector Active Share and Market Cap Quintile (MCAPQ) Active Share.
	To see why, we took a look at data for the first date (date of January 3rd, 2007 ) of a given dataset. Figure 6 shows the sum of the weights for each sector and MCㅁAPQ to satisfy Constraints 6 and 7. Because $w_\text{bench} $ is different for each asset, the scope varies depending on what asset is included in the sector and the MCPQA. In other words, when the sum of each sector and MCAPQ is sensitive, it becomes a difficult constraint to satisfy. Therefore, if the constraints 6 and 7 are violated in the process of improving the cardinality set, adding deficient sector and mcapq assets is prevent infeasible. 
\begin{figure}[h]
	\begin{center}
	\begin{tikzpicture}
	\begin{axis}[
	width=9cm,
	height=6cm,
	ybar stacked,
	bar width=10pt,
	enlargelimits=0.15,
	legend style={at={(0.5,-1)},
		anchor=north,legend columns=-1},
	ylabel={\textbf{The range of $\sum_{i \in X}w_i$}},
	xlabel={\textbf{Sector $X$}},
	xlabel near ticks,
	ymin=0,
	ymax=0.5,
	symbolic x coords={Indust., Tele.\&Ser., Cons.\&Disc., Health., 
		Energe, Cons.\&Sta., Finan.,Mater.,Utili.,IT},
	xtick=data,
	x tick label style={rotate=45,anchor=east},
	  xticklabel style = {font=\scriptsize}
	]
	
	\addplot+[ybar,color=black,draw=none,fill=white ] plot coordinates {(Indust., 0.01)
		(Tele.\&Ser., -0.07)
		(Cons.\&Disc., 0.02)
		(Health., 0.02)
		(Energe, -0.004)
		(Cons.\&Sta., -0.006)
		(Finan., 0.12)
		(Mater., -0.07)
		(Utili., -0.07)
		(IT, 0.05)
	};
	\addplot+[ybar,color=black, draw=none,fill=lightgray] plot coordinates { (Indust., 0.21)
		(Tele.\&Ser., 0.12)
		(Cons.\&Disc., 0.22)
		(Health., 0.22)
		(Energe, 0.2)
		(Cons.\&Sta., 0.19)
		(Finan., 0.32)
		(Mater., 0.13)
		(Utili., 0.13)
		(IT, 0.25)
	};
	
	\end{axis}
	\end{tikzpicture}
		 	\quad
	\begin{tikzpicture}
	\begin{axis}[
		width=6cm,
	height=6cm,
	ybar stacked,
	bar width=10pt,
	enlargelimits=0.2,
	legend style={at={(0.5,-1)},
	anchor=north,legend columns=-1},
	xlabel={\textbf{MCAPQ $X$}},
	xlabel near ticks,
	ymin=0,
	ymax=0.7,
	symbolic x coords={MCAPQ1, MCAPQ2, MCAPQ3, MCAPQ4, MCAPQ5},
	xtick=data,
	 xticklabel style = {font=\scriptsize},
	x tick label style={rotate=45,anchor=east},
	]
	\addplot+[ybar,color=black,draw=none,fill=white ] plot coordinates {(MCAPQ1, 0.613783144)
		(MCAPQ2, 0.066327628)
		(MCAPQ3, -0.022)
		(MCAPQ4, -0.07)
		(MCAPQ5, -0.086)};
	\addplot+[ybar,color=black, draw=none,fill=lightgray] plot coordinates { (MCAPQ1, 0.81378314399999996)
		(MCAPQ2, 0.26632762799999998)
		(MCAPQ3, 0.17779934600000002)
		(MCAPQ4, 0.12814390000000001)
		(MCAPQ5, 0.113946004)	};
	\end{axis}
	\end{tikzpicture}
	\caption{The sum of the weights for each Sector and MCAPQ}
		\end{center}
\end{figure}

\end{itemize}



\textbf{Analytics Solution and Results}
\begin{itemize}
\item[] In this section, we report results of computational experiments for solving portfolio optimization with the methodology we present. First, we show how to set the parameter $\lambda$ to be considered in the problem in order to obtain the optimal solution before proceeding with the experiment. Also, we present how rebalancing reflects in our proposed model. Finally, the results of the computational experiments with the determined parameters are present.

	\item[\textbf{1.}] \textbf{Parameter Setting}
	\item[]To set parameters, various experimental analyzes are required with real data. However, there is a limit to obtaining the real data such as $\alpha, \Omega, \beta$, and bench weight from the historical data. we assume that the year 2007 is a parameter adjustment period, and analyzed the data in 2007. 
	
	\item[] \underline{\textbf{The Parameter $\lambda$}}: Parameter $ \lambda $ reflects how much return will be considered relative to risk in objective function. When the value of $ \lambda $ is large, the return is considered a lot, and when the $ \lambda $ value is low, the risk is considered more. We used 2007 data to analyze whether the $d^{T} \alpha$, which represents a return on objective, has an influence on the overall return index ($r_{opt}$) as we adjusted the $\lambda$. In other words, as the lambda value increases, more revenue will be taken into account, so that it can be inferred that the $r_{opt}$  is more affected by return or risk.
	%실험 
	%실험결과: 어떤 지표와도 상관이 없다.
	%원인 1) risk 와 return의 범위 : 25언저리 vs 0.5 (람다가10일때) 
	%		2) 실제 return과 r_opt와의 관계가 없었음
	%결론 : 큰 차이가 없으므로 default 값인 1을 사용
	
	\item[\textbf{2.}] \textbf{Rabalancing}
	\item[]Portfolio turnover depends on how many assets in the portfolio chance from rebalance to rebalance, and the value of turnover has effects on Information Ratio(IR). Therefore, it is required to consider turnover in the portfolio optimization. We apply turnover to the both GAN for finding initial cardinality set and QCQP problem for determining the weight of the selected asset. For applying it to GAN, the calculated $\text{turnover}_{t} (\sum_{i \ inN}|w_{i,t}-w_{i,t-1}^{pre}|)$was directly taken into account in the objective function. On the other hand,for applying it to MIP, it should be linearlized. We reflect turnover into the objective function in MIP as follows: 
	\begin{align*}
	\text{(MIP)} \quad \min \quad & d^T\Omega d  -  \lambda d^{T}\alpha - \omega \sum_{i \in N}o_i \quad  \quad \quad\text{(2*)}\\
	\text{s.t } \quad & (3) - (11)\\
	& o_i \geq w_{i,t}-w_{i,t-1}^{pre}, \quad \forall{i \in N} \\
	& o_i \geq -w_{i,t}+w_{i,t-1}^{pre}, \quad \forall{i \in N} 
	\end{align*}
	The decision variables $o_i$ is the difference between $w_i$ of previous period(t-1) and $w_i$ of current(t) period for all asset $i \in N$. Because of constraints added, $ w_{i,t}-w_{i,t-1}^{pre}$ is positive for all asset $i \in N$. The parameter $\omega$ represents the weight of turnover for objective value considering risk and return. 
	\item[] \underline{\textbf{The Parameter $ \omega$}}: 
	%실험내용
	%실험결과 및 결론
	
	\item[\textbf{3.}] \textbf{Computational Experiments}
	\item[] \underline{\textbf{Experiment environment}} : The experiments were performed on an Intel Core 3.5 GHz PC with 32GB memory and ILOG CPLEX 12.6 were used as an MIP solver, and Pytorch for Python 3.6 was used as a deep learning framework. 
	
	
	\item[] \underline{\textbf{The Portfolio Performance Statistics}} : 
		\begin{center}
			\textbf{Portfolio Performance Statistics}\vspace*{-14pt}
		\end{center}
	
		\begin{table}[htbp]
			\def\arraystretch{1.4}
			\begin{center}
				\begin{tabular}{|l|c|c|}
					\hline
					\textbf{2007-01-01 to 2016-12-31}& 
					\textbf{Portfolio}& 
					\textbf{Benchmark} \\
					\hline
					Cumulative Return& 
					{\%}& 
					{\%} \\
					\hline
					Annualized Return& 
					{\%}& 
					{\%} \\
					\hline
					Annualized Excess Return& 
					{\%}& 
					-- \\
					\hline
					Annualized Tracking Error& 
					{\%}& 
					-- \\
					\hline
					Sharpe Ratio& 
					& 
					\\
					\hline
					Information Ratio& 
					& 
					-- \\
					\hline
				\end{tabular}
				\label{tab1}
			\end{center}
		\end{table}


\item[\textbf{4.}] \textbf{Robust Optimization} 
\item[]The results obtained earlier are derived by deterministic values which are $\alpha$ and $\Omega$. However, this is a parameter value with uncertainty, so we figure out the change of $\alpha$ and $\Omega$ in the historical data.  We first tried to secure the value of risk and alpha score for 2007 by analyzing the historical data. The analyzed data is obtained from Yahoo Finance and is historical data from 1970 to 2017. For the first time, we are looking at the risk of an asset in 2006 with extensive historical data. We obtained data on close stock prices for approximately 400 assets from historical data among 492 assets included in January 3, 2007 in S \& P 500 and derived a cross covariance matrix. And then, the correlation between the covariance matrix and the full matrix of $\Omega$ was derived. We expected a large correlation between the two matrices, but the result was not. It is judged that it is difficult to deduce $\Omega$ value from the covariance matrix derived from historical data. Therefore, we obtain the range of parameter change for each asset with the first year data received from the Principal and reflect it in the following robust optimization. %왜 \alpha만 고려하는가? 쉽게 고려할 수 있는 부분?
Let set $U := \{ \tilde \alpha  \mid \tilde \alpha_i = \hat{\alpha_i} + \bar{\alpha_i}\gamma_i , \quad -1 \leq \gamma_i\leq 1 , \quad \sum_{i} |\gamma_i| = \Gamma \}$. We may a simplified formulation because the worst-case only occurs when $\tilde{d_i} =\hat{d_i} - \bar{\alpha_i}, \forall i \in N (\text{i.e} \gamma_i = -1)$. Let set $U := \{ \tilde \alpha  \mid \tilde \alpha_i = \hat{\alpha_i} + \bar{\alpha_i}\gamma_i , \quad 0 \leq \gamma_i\leq 1 , \quad \sum_{i} \gamma_i = \Gamma \}$. For a given $\alpha$, the objective function considering robustness can be expressed as follows :
\begin{align*}
\text{(Robust)} 
\text{} \quad \min_{\tilde \alpha \in U } d^T \tilde{\alpha} &= \min \sum_{i} d^T (\hat{\alpha_i} - \bar{\alpha_i}\gamma_i ) \\
& \sum_{i} \gamma_i \leq \bar{\alpha_i} d_i , \quad \forall i \in N\\
& \quad 0 \leq \gamma_i\leq 1 , \quad \forall i \in N
\end{align*}
However, %이건 바로 풀수가 없어.... 
Thus, %dual로...
\begin{align*}
\text{(Robust-Dual) $\Rightarrow$} \quad \max\quad & \Gamma  \pi + \sum_{i} \theta_i  \\
\text{s.t} \quad & \pi + \theta_i \leq   \bar{\alpha} d_i, \quad \forall i \in N \\
& \pi \geq 0 \quad \\
&  \theta_i \geq 0, \quad \forall i \in N 
\end{align*}
%이 문제 전체를 다시 표현해보면...
\begin{align*}
\text{(ALL)} \quad \min \quad & d^T\Omega d  -  \lambda (\Gamma \pi + \sum_{i} \theta_i  ) \\
\text{s.t } \quad & (1) - (11)\\
&\pi + \theta_i \leq   \bar{\alpha} d_i, \quad \forall i \in N \\
&\pi \geq 0 \\
& \theta_i \geq 0, \quad \forall i \in N 
\end{align*}
\end{itemize}

\textbf{References}

\bgroup
\parskip0pt

Please follow guidelines in the \textit{Chicago Manual of Style,} 16$^{\text{th}}$ Edition. Here are examples: 

\begin{itemize}
\item[--] Journal article: Flynn J, Gartska SK (1990) A dynamic inventory model with periodic auditing. \textit{Oper. Res.} 38(6):1089--1103. 
\item[--] Book: Makridakis S, Wheelwright SC, McGee VE (1983) \textit{Forecasting: Methods and Applications}, 2nd ed. (John Wiley {\&} Sons, New York). 
\item[--] Edited Book: Martello S, Toth P (1979) The 0-1 knapsack problem. Christofides N, Mingozzi A, Sandi C, eds. \textit{Combinatorial Optimization} (John Wiley {\&} Sons, New York), 237--279.
\item[--] Online reference, fictional example: American Mathematical Institute (2005) Better predictors of geospatial variability. Retrieved June 14, 2005, \underline {www.mathematicsinstitute}.


\end{itemize}
\egroup

\end{document}
